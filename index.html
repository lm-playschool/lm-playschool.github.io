<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LM Play School Challenge</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            padding: 50px;
            background-color: #f4f4f4;
        }
        h1 {
            color: #333;
        }
        p {
            color: #666;
            font-size: 18px;
        }
        .container {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            display: inline-block;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Welcome to the LM Play School Challenge!</h1>
        <p>Learning through play and interaction.</p>
        <p>Stay tuned for updates!</p>
    </div>

<p>  The PlaySchool-LM challenge  aims at fostering research on training regimes that are best suited for creating agentic conversational language models (ACLMs).  It therefore targets significant advances in the efficient development of collaborative AI systems. We are requesting sponsorship support to develop and run the challenge. 

  The challenge builds on the observation that current training is partially ineffective, in that the resulting models lack key collaborative interactional competences that human language users have. 

  The  Playschool-LM challenge therefore poses the following questions: How can we efficiently improve pre-trained LMs towards modelling a collaborative language understander and user, given a fixed token post-training  budget?  Specifically, it asks ``What role can structured interaction with a teacher model play in the training process?’’ </p>


  <p>The challenge organisers provide a setup (the Playpen) for conducting such structured interactions, which offers as learning signal both linguistic feedback from a teacher (clarification requests, corrections) as well as feedback from the (interaction) environment (task success or failure), and which provides some control over the training process to the learner. The organisers also provide an evaluation pipeline that operationalises the notion of a "collaborative language understander and user (CLU2)"  by mixing established reference-based language understanding evaluation datasets (e.g., MMLU) with novel interactional tests, ensuring that both "internal" skills such as reasoning, understanding, as well as "external" skills such as conversational collaboration and multi-turn coherence establishment are tested.

All participants in the challenge start with the same base model, which will be the best performing (at the time of the start of the challenge) model in the ~10B parameters class, pre-trained to be able to follow instructions. (This motivates the name of the challenge: whereas the BabyLM challenge is about starting training from scratch with a certain budget, the PlaySchool-LM challenge is about improving a pre-trained model with a certain budget.)

Participants are free to use whatever method or data they see fit to achieve improvements. Evaluation will be on the CLU2 pipeline mentioned above; with a special prize reserved for the highest improvement on the "collaborative language use" dimension. Use of the Playpen is encouraged, but not required. Entries to the competition can be (but do not have to be) pure fine-tunes of the base model; agentive systems that wrap the model in additional code (e.g., for introspection or self-regulation) are also welcome.</p>


<H2>Organizers</H2>
  <UL>
<LI> Raffaella Bernardi (Uni Trento)</LI>
<LI>Raquel Fernández (Uni Amsterdam)</LI>
<LI>Mario Giulianelli (ETH Zurich)</LI>
<LI>Alexander Koller (Saarland Uni)</LI>
<LI>Oliver Lemon (Heriot-Watt Uni)</LI>
<LI>David Schlangen (Uni Potsdam)</LI>
<LI>Alessandro Suglia (Heriot-Watt Uni)</LI>
   <LI>ALL THE OTHER NAMES</LI>
  </UL>
</body>
  
</html>
